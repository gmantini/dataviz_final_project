---
title: "Mini Project 3"
subtitle: "Visualizing Text and Distributions"
output: 
  html_document:
    keep_md: true
---

# Data Visualization Project 03


In this exercise you will explore methods to visualize text data and/or practice how to recreate charts that show the distributions of a continuous variable. 


## Visualizing Text Data

Review the set of slides (and additional resources linked in it) for visualizing text data: https://www.reisanar.com/slides/text-viz#1

Choose any dataset with text data, and create at least one visualization with it. For example, you can create a frequency count of most used bigrams, a sentiment analysis of the text data, a network visualization of terms commonly used together, and/or a visualization of a topic modeling approach to the problem of identifying words/documents associated to different topics in the text data you decide to use. 

Make sure to include a copy of the dataset in the `data/` folder, and reference your sources if different from the ones listed below:

- [Billboard Top 100 Lyrics](https://github.com/reisanar/datasets/blob/master/BB_top100_2015.csv)

- [RateMyProfessors comments](https://github.com/reisanar/datasets/blob/master/rmp_wit_comments.csv)

- [FL Poly News 2020](https://github.com/reisanar/datasets/blob/master/poly_news_FL20.csv)

- [FL Poly News 2019](https://github.com/reisanar/datasets/blob/master/poly_news_FL19.csv)

(to get the "raw" data from any of the links listed above, simply click on the `raw` button of the GitHub page and copy the URL to be able to read it in your computer using the `read_csv()` function)

```{r, message = FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(gutenbergr)
library(sentimentr)
library(wordcloud)
library(textdata)
library(reshape2)
```


# Sentiment Analysis

```{r}
lyrics <- read_csv("https://raw.githubusercontent.com/reisanar/datasets/master/BB_top100_2015.csv");lyrics
```

```{r}
lyrics <- select(lyrics, -Year, -Source); head(lyrics)
```

### Pre-processing

```{r}
top_100 <- lyrics %>%
  unnest_tokens(word, Lyrics, token = "words") %>%
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))

top_100
```

### Sentiment analysis with afinn lexicon

```{r}
top_100 %>%
  inner_join(get_sentiments("afinn")) %>%
  count(Song, value) %>% 
  spread(value, n, fill = 0)
```
 

```{r}
top_100 %>%
  group_by(word) %>%
  summarise(uses = n()) %>%
  arrange(desc(uses))
```

### The weeknd song
```{r}
the_weeknd <- lyrics %>% 
 filter(Artist == "the weeknd") %>% 
 unnest_tokens(word, Lyrics) %>% 
 anti_join(stop_words)
head(the_weeknd)
```

Top words in the weeknd's songs
```{r}
the_weeknd %>%
  group_by(word) %>%
  summarise(uses = n()) %>%
  arrange(desc(uses))
```

### Word Cloud



```{r}
top_100_wordcloud <- the_weeknd %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words=100))
```


### Word cloud with *bing* lexicon



```{r}
the_weeknd %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word~sentiment, value.var ="n", fill=0) %>%
  comparison.cloud(colors = c("red","blue"), max.words = 100)
```
